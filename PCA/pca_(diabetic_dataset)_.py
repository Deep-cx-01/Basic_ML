# -*- coding: utf-8 -*-
"""PCA (Diabetic Dataset) .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dJt9atod_684KxcScqfw2iH6-8iTJrMT

Importing the Required packages.
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

"""Loading the dataset.

"""

df= pd.read_csv('/content/diabetes.csv')
df.sample(7)

"""Performing EDA"""

df.isnull().sum()

# Target columns and correleation coefficeint

df.corr()
plt.figure(figsize=(7,7))
ax = sns.heatmap(df.corr(), annot=True)
plt.savefig('correlation-coefficient.jpg')

plt.show()

# Descriptive nature of the dataset
df.describe()

# Data Imputation for zero values of important factors
df['Insulin'] = df['Insulin'].replace(0, df['Insulin'].median())
df['Pregnancies'] = df['Pregnancies'].replace(0, df['Pregnancies'].median())
df['Glucose'] = df['Glucose'].replace(0, df['Glucose'].mean())
df['BloodPressure'] = df['BloodPressure'].replace(0, df['BloodPressure'].mean())
df['SkinThickness'] = df['SkinThickness'].replace(0, df['SkinThickness'].median())
df['BMI'] = df['BMI'].replace(0, df['BMI'].mean())
df['DiabetesPedigreeFunction'] = df['DiabetesPedigreeFunction'].replace(0, df['DiabetesPedigreeFunction'].median())
df['Age'] = df['Age'].replace(0, df['Age'].median())

# We only required input features for PCA
df = df.drop(columns=['Outcome'], axis=1)

# outliers Detection and removal
fig, ax = plt.subplots(figsize = (7, 7))
sns.boxplot(data = df, ax=ax)
plt.savefig('boxPlot.jpg')

# now removing or treating the outliers
cols = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age']
for col in cols:
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    mask = (df[col] >= lower_bound) & (df[col] <= upper_bound)

# filter dataset to remove outliers
df_outliers=df[mask];

df.describe()

"""**NOTE:** Here we can observe that the mean and varinace are not of standard formats so we try to standarized the data first.<br>


"""

## convert the data into standard scaler form
## mean = 0 and standard deviation = 1
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
dataScaled = scaler.fit_transform(df)
dataScaled = pd.DataFrame(dataScaled)
dataScaled.describe()

"""Now comes the further steps of PCA.
1. Standarization
2. Finding Covariance matrix
3. Eigen vectors
4. Choosing no of components
5. Forming principle Components
"""

# Finding the Covariance Matrix
cov_matrix =dataScaled.T @dataScaled/dataScaled.shape[0]
cov_matrix

# Evaluate the EigenValues and eigenvectors using the Covariance Matrix
eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)

eigenvalues

eigenvectors

"""**Calculate the explained variance ratio**<br>
The explained variance ratio is a critical step in PCA that indicates the proportion of the dataset's variance captured by each principal component. The explained variance ratio helps in understanding how much information (variance) is retained by each principal component. In the example above, pca.explained_variance_ratio_ returns an array of the variance ratios for the selected components.
"""

from sklearn.decomposition import PCA
pca = PCA()
principalComponent = pca.fit_transform(dataScaled)
pca.explained_variance_ratio_

# Visualizing Cumulative Explained Variance in PCA
import matplotlib.pyplot as plt
plt.plot(np.cumsum(pca.explained_variance_ratio_))
plt.xlabel("Dimensions")
plt.ylabel("Explained Variance Ratio")
plt.savefig("ScreenPlot.png")
plt.show()

"""**NOTE:** This graph shows that for 90 percent of data we need to define the principle components i.e(6)."""

# Defining Principle Components
## PC1 data
PC1_data = dataScaled @ eigenvectors[:, 0]
## PC2 data
PC2_data = dataScaled @ eigenvectors[:, 1]
## PC3 data
PC3_data = dataScaled @ eigenvectors[:, 2]
## PC4 data
PC4_data = dataScaled @ eigenvectors[:, 3]
## PC5 data
PC5_data = dataScaled @ eigenvectors[:, 4]
## PC6 data
PC6_data = dataScaled @ eigenvectors[:, 7]

"""Now we are using the other method .

**PCA Implementation via the sklearn library**<br>
basically all the steps are same as we proceed just they are performed in beside in this funciton in the library.
"""

from sklearn.decomposition import PCA
pca = PCA(n_components=3)
pca.fit_transform(dataScaled)

"""The main question arrives that if we observe the valyes of eigen  vectors we find step and by step and manually are just negative from we find uding predefined function in the library .

**Answer:**<br>
Does This Affect PCA Results?<br>
No, this does not affect the actual results of PCA. Hereâ€™s why:

1. **Eigenvalue Magnitude is the Same:** The magnitude of the eigenvalues, which represents the variance captured by each principal component, remains unchanged regardless of the sign of the corresponding eigenvectors.<br>
2. **Principal Component Scores Remain Consistent:** The scores or projections of the data onto the principal components are still the same, up to a sign flip. Flipping the sign of a component does not change the structure or relative relationships in the data. It merely reverses the axis direction.<br>
3. **Interpretation of PCs is Still Valid:** Whether the components have positive or negative signs, the overall variance captured by each component remains the same. The eigenvectors still point in the direction of maximum variance, and their magnitudes still represent the strength of the variance in that direction.
"""

